{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# #### VAE synthetic data generation \n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "#!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# Load the existing dataset from 'data.csv' (adjust the file path as needed)\n",
    "existing_data = pd.read_csv('Crop_recommendation.csv')\n",
    "existing_data.head()\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def Output(s):\n",
    "    if s == \"rice\":\n",
    "        return 0\n",
    "    elif s== \"maize\":\n",
    "        return 1\n",
    "    elif s==\"chickpea\":\n",
    "        return 2\n",
    "    elif s==\"kidneybeans\":\n",
    "        return 3\n",
    "    if s == \"mothbeans\":\n",
    "        return 4\n",
    "    elif s== \"mungbean\":\n",
    "        return 5\n",
    "    elif s==\"blackgram\":\n",
    "        return 6\n",
    "    elif s==\"lentil\":\n",
    "        return 7\n",
    "    if s == \"pomegranate\":\n",
    "        return 8\n",
    "    elif s== \"banana\":\n",
    "        return 9\n",
    "    elif s==\"mango\":\n",
    "        return 10\n",
    "    elif s==\"grapes\":\n",
    "        return 11\n",
    "    if s == \"watermelon\":\n",
    "        return 12\n",
    "    elif s== \"muskmelon\":\n",
    "        return 13\n",
    "    elif s==\"apple\":\n",
    "        return 14\n",
    "    elif s==\"orange\":\n",
    "        return 15\n",
    "    elif s==\"papaya\":\n",
    "        return 16\n",
    "    if s == \"coconut\":\n",
    "        return 17\n",
    "    elif s== \"cotton\":\n",
    "        return 18\n",
    "    elif s==\"jute\":\n",
    "        return 19\n",
    "    elif s==\"coffee\":\n",
    "        return 20\n",
    "    elif s==\"pigeonpeas\":\n",
    "        return 21\n",
    "\n",
    "existing_data.label = existing_data.label.map(Output)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "existing_data.head()\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "existing_data.isnull().sum()\n",
    "#existing_data[\"MULTIPLIER\"] = existing_data[\"MULTIPLIER\"].fillna(0.0)\n",
    "\n",
    "le_n = LabelEncoder()\n",
    "existing_data['N'] = le_n.fit_transform(existing_data['N'])\n",
    "\n",
    "le_p = LabelEncoder()\n",
    "existing_data['P'] = le_p.fit_transform(existing_data['P'])\n",
    "\n",
    "le_k = LabelEncoder()\n",
    "existing_data['K'] = le_k.fit_transform(existing_data['K'])\n",
    "\n",
    "le_temperature = LabelEncoder()\n",
    "existing_data['temperature'] = le_temperature.fit_transform(existing_data['temperature'])\n",
    "\n",
    "le_humidity = LabelEncoder()\n",
    "existing_data['humidity'] = le_humidity.fit_transform(existing_data['humidity'])\n",
    "\n",
    "le_ph = LabelEncoder()\n",
    "existing_data['ph'] = le_ph.fit_transform(existing_data['ph'])\n",
    "\n",
    "le_rainfall = LabelEncoder()\n",
    "existing_data['rainfall'] = le_rainfall.fit_transform(existing_data['rainfall']) \n",
    "\n",
    "le_label = LabelEncoder()\n",
    "existing_data['label'] = le_label.fit_transform(existing_data['label']) \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "existing_data_scaled = scaler.fit_transform(existing_data)\n",
    "X_train, X_test = train_test_split(existing_data_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "latent_dim = 8\n",
    "# Encoder network\n",
    "encoder_input = keras.Input(shape=(existing_data_scaled.shape[1],))\n",
    "encoder_hidden = keras.layers.Dense(64, activation='relu')(encoder_input)\n",
    "z_mean = keras.layers.Dense(latent_dim, name='z_mean')(encoder_hidden)\n",
    "z_log_var = keras.layers.Dense(latent_dim, name='z_log_var')(encoder_hidden)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(tf.shape(z_mean)[0], latent_dim), mean=0., stddev=1.)\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "z = keras.layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "# Decoder network\n",
    "decoder_hidden = keras.layers.Dense(64, activation='relu')(z)\n",
    "decoder_output = keras.layers.Dense(existing_data_scaled.shape[1], activation='sigmoid')(decoder_hidden)\n",
    "\n",
    "\n",
    "# Create the VAE model\n",
    "vae = keras.Model(encoder_input, decoder_output)\n",
    "\n",
    "# Define the custom loss function (binary cross-entropy)\n",
    "def custom_bce_loss(y_true, y_pred):\n",
    "    #Compute binary cross-entropy loss\n",
    "    bce_loss = keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    return bce_loss\n",
    "\n",
    "\n",
    "# Compile the VAE model with custom loss function\n",
    "vae.compile(optimizer='adam', loss=custom_bce_loss)\n",
    "\n",
    "\n",
    "# Train the VAE model\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "vae.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, X_test))\n",
    "#Generate synthetic data by sampling from the VAE's latent space\n",
    "num_samples = len(existing_data)\n",
    "synthetic_data_encoded = np.random.normal(0, 1, size=(num_samples, latent_dim))\n",
    "synthetic_data_decoded = vae.predict(synthetic_data_encoded)\n",
    "\n",
    "#Inverse transform to obtain synthetic data in the original scale\n",
    "synthetic_data = scaler.inverse_transform(synthetic_data_decoded)\n",
    "\n",
    "#Create a DataFrame for the synthetic data\n",
    "synthetic_data = pd.DataFrame(synthetic_data, columns=existing_data.columns)\n",
    "\n",
    "#Display the first few rows of the synthetic data\n",
    "print(synthetic_data.head())\n",
    "\n",
    "\n",
    "#Save the synthetic data to a CSV file\n",
    "#synthetic_data.to_csv('synthetic_data_vae.csv', index=False)\n",
    "\n",
    "synthetic_data['N'] = le_n.inverse_transform(synthetic_data['N'].astype(int))\n",
    "synthetic_data['P'] = le_p.inverse_transform(synthetic_data['P'].astype(int))\n",
    "synthetic_data['K'] = le_k.inverse_transform(synthetic_data['K'].astype(int))\n",
    "synthetic_data['temperature'] = le_temperature.inverse_transform(synthetic_data['temperature'].astype(int))\n",
    "synthetic_data['humidity'] = le_humidity.inverse_transform(synthetic_data['humidity'].astype(int))\n",
    "synthetic_data['ph'] = le_ph.inverse_transform(synthetic_data['ph'].astype(int))\n",
    "synthetic_data['rainfall'] = le_rainfall.inverse_transform(synthetic_data['rainfall'].astype(int))\n",
    "synthetic_data['label'] = le_label.inverse_transform(synthetic_data['label'].astype(int))\n",
    "\n",
    "\n",
    "synthetic_data.head()\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "synthetic_data.shape\n",
    "\n",
    "\n",
    "# ### GAN Synthetic data generation \n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# installing the package\n",
    "#!pip install ctgan\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "gan_data = pd.read_csv('Crop_recommendation.csv')\n",
    "gan_data.head()\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "gan_data.label = gan_data.label.map(Output)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "from ctgan import CTGAN\n",
    "columns= list(gan_data.columns)\n",
    "ctgan= CTGAN(verbose= True, epochs=30)\n",
    "ctgan.fit(gan_data, columns)\n",
    "new_sample= ctgan.sample(100)\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "new_sample.shape\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "new_sample.head()\n",
    "\n",
    "\n",
    "# #### Synthetic Data through program \n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import chi2_contingency\n",
    "from ctgan import CTGAN\n",
    "from scipy.stats import kstest\n",
    "from table_evaluator import TableEvaluator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "from sklearn.impute import KNNImputer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None\n",
    "\n",
    "\n",
    "# #### Epochs 200 \n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "data = pd.read_csv('Crop_recommendation.csv')\n",
    "data.head()\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "data = data.rename(columns={\"N\":\"Nitogen\",\"P\":\"Phosporous\",\"K\":\"Potasium\",\"ph\":\"Potential of Hydrogen\"})\n",
    "le_label = LabelEncoder()\n",
    "data[\"label\"] = le_label.fit_transform(data[\"label\"])\n",
    "data.head()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def _df(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    for c in range(df.shape[1]):\n",
    "        mapping = {df.columns[c]: c}\n",
    "        df = df.rename(columns=mapping)\n",
    "    return df\n",
    "\n",
    "X = (data.drop(columns=[\"label\"])).values\n",
    "y = (data[\"label\"]).values\n",
    "X = KNNImputer().fit_transform(X)\n",
    "data = _df(StandardScaler().fit_transform(np.column_stack((X, y))))\n",
    "\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "class Gan():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.n_epochs = 200\n",
    "    def _noise(self):\n",
    "        noise = np.random.normal(0, 1, self.data.shape)\n",
    "        return noise\n",
    "    def _generator(self):\n",
    "        model = tf.keras.Sequential(name=\"Generator_model\")\n",
    "        model.add(tf.keras.layers.Dense(15, activation='relu',\n",
    "                    kernel_initializer='he_uniform',\n",
    "                    input_dim=self.data.shape[1]))\n",
    "        model.add(tf.keras.layers.Dense(30, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "        self.data.shape[1], activation='linear'))\n",
    "        return model\n",
    "\n",
    "    def _discriminator(self):\n",
    "        model = tf.keras.Sequential(name=\"Discriminator_model\")\n",
    "        model.add(tf.keras.layers.Dense(25, activation='relu',\n",
    "                    kernel_initializer='he_uniform',\n",
    "                    input_dim=self.data.shape[1]))\n",
    "        model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "        # sigmoid => real or fake\n",
    "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "           optimizer='adam',\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "      # defining the combined generator and discriminator model,for updating the generator\n",
    "    def _GAN(self, generator, discriminator):\n",
    "        discriminator.trainable = False\n",
    "        generator.trainable = True\n",
    "        model = tf.keras.Sequential(name=\"GAN\")\n",
    "        model.add(generator)\n",
    "        model.add(discriminator)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "        return model\n",
    "\n",
    "      # train the generator and discriminator\n",
    "    def train(self, generator, discriminator, gan):\n",
    "        for epoch in range(self.n_epochs):\n",
    "            generated_data = generator.predict(self._noise())\n",
    "            labels = np.concatenate([np.ones(self.data.shape[0]), np.zeros(self.data.shape[0])])\n",
    "            X = np.concatenate([self.data, generated_data])\n",
    "            discriminator.trainable = True\n",
    "            d_loss , _ = discriminator.train_on_batch(X, labels)\n",
    "            noise = self._noise()\n",
    "            g_loss = gan.train_on_batch(noise, np.ones(self.data.shape[0]))\n",
    "            print('>%d, d1=%.3f, d2=%.3f' %(epoch+1, d_loss, g_loss))\n",
    "        return generator\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "model = Gan(data=data)\n",
    "generator = model._generator()\n",
    "descriminator = model._discriminator()\n",
    "gan_model = model._GAN(generator=generator, discriminator=descriminator)\n",
    "trained_model = model.train(generator=generator, discriminator=descriminator, gan=gan_model)\n",
    "noise = np.random.normal(0, 1, data.shape) \n",
    "new_data = _df(data=trained_model.predict(noise))\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "sns.heatmap(data.corr(), annot=True, ax=ax[0], cmap=\"Blues\")\n",
    "sns.heatmap(new_data.corr(), annot=True, ax=ax[1], cmap=\"Blues\")\n",
    "ax[0].set_title(\"Original Data\")\n",
    "ax[1].set_title(\"synthetic Data\")\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "ax[0].scatter(data.iloc[:, 0], data.iloc[:, 1])\n",
    "ax[1].scatter(new_data.iloc[:, 0], new_data.iloc[:, 1])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "ax[1].set_title(\"synthetic Data\")\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "table_evaluator=TableEvaluator(data, new_data)\n",
    "table_evaluator.visual_evaluation()\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "\n",
    "# def _df(data):\n",
    "#     df = pd.DataFrame(data)\n",
    "#     for c in range(df.shape[1]):\n",
    "#         mapping = {df.columns[c]: c}\n",
    "#         df = df.rename(columns=mapping)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# X = (data.drop(columns=[\"label\"])).values\n",
    "# y = (data[\"label\"]).values\n",
    "\n",
    "\n",
    "# X = KNNImputer().fit_transform(X)\n",
    "# data = _df(StandardScaler().fit_transform(np.column_stack((X, y))))\n",
    "\n",
    "\n",
    "# # In[19]:\n",
    "\n",
    "\n",
    "# tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "class Gan():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.n_epochs = 20\n",
    "    def _noise(self):\n",
    "        noise = np.random.normal(0, 1, self.data.shape)\n",
    "        return noise\n",
    "    def _generator(self):\n",
    "        model = tf.keras.Sequential(name=\"Generator_model\")\n",
    "        model.add(tf.keras.layers.Dense(100, activation='relu',kernel_initializer='he_uniform',input_dim=self.data.shape[1]))\n",
    "        model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.data.shape[1], activation='linear'))\n",
    "        return model\n",
    "    def _discriminator(self):\n",
    "        model = tf.keras.Sequential(name=\"Discriminator_model\")\n",
    "        model.add(tf.keras.layers.Dense(100, activation='relu',kernel_initializer='he_uniform',input_dim=self.data.shape[1]))\n",
    "        model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "        return model\n",
    "    def _GAN(self, generator, discriminator):\n",
    "        discriminator.trainable = False\n",
    "        generator.trainable = True\n",
    "        model = tf.keras.Sequential(name=\"GAN\")\n",
    "        model.add(generator)\n",
    "        model.add(discriminator)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "        return model\n",
    "    def train(self, generator, discriminator, gan):\n",
    "        for epoch in range(self.n_epochs):\n",
    "            generated_data = generator.predict(self._noise())\n",
    "            labels = np.concatenate([np.ones(self.data.shape[0]), np.zeros(self.data.shape[0])])\n",
    "            X = np.concatenate([self.data, generated_data])\n",
    "            discriminator.trainable = True\n",
    "            d_loss , _ = discriminator.train_on_batch(X, labels)\n",
    "            noise = self._noise()\n",
    "            g_loss = gan.train_on_batch(noise, np.ones(self.data.shape[0]))\n",
    "            print('>%d, d1=%.3f, d2=%.3f' %(epoch+1, d_loss, g_loss))\n",
    "        return generator\n",
    "\n",
    "model = Gan(data=data)\n",
    "generator = model._generator()\n",
    "descriminator = model._discriminator()\n",
    "gan_model = model._GAN(generator=generator, discriminator=descriminator)\n",
    "trained_model = model.train(generator=generator, discriminator=descriminator, gan=gan_model)\n",
    "\n",
    "noise = np.random.normal(0, 1, data.shape) \n",
    "new_data = _df(data=trained_model.predict(noise))\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "sns.heatmap(data.corr(), annot=True, ax=ax[0], cmap=\"Blues\")\n",
    "sns.heatmap(new_data.corr(), annot=True, ax=ax[1], cmap=\"Blues\")\n",
    "ax[0].set_title(\"Original Data\")\n",
    "ax[1].set_title(\"synthetic Data\")\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "ax[0].scatter(data.iloc[:, 0], data.iloc[:, 1])\n",
    "ax[1].scatter(new_data.iloc[:, 0], new_data.iloc[:, 1])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "ax[1].set_title(\"synthetic Data\")\n",
    "\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "table_evaluator=TableEvaluator(data, new_data)\n",
    "table_evaluator.visual_evaluation()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv('Crop_recommendation (1).csv')\n",
    "data.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "data = data.rename(columns={\"N\":\"Nitogen\",\"P\":\"Phosporous\",\"K\":\"Potasium\",\"ph\":\"Potential of Hydrogen\"})\n",
    "le_label = LabelEncoder()\n",
    "data[\"label\"] = le_label.fit_transform(data[\"label\"])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "columns=list(data.columns)\n",
    "ctgan = CTGAN(verbose=True, epochs = 50)\n",
    "ctgan.fit(data,columns)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "synthetic_data = ctgan.sample(100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
